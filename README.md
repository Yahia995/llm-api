# LLM API with FastAPI (Async + Celery Version)

This project demonstrates a minimal Large Language Model (LLM) API built using **FastAPI** and connected to a locally running LLM served by **Ollama**.

The current version introduces **background task processing using Celery and Redis**, enabling non-blocking LLM inference and improved scalability.

The project follows an **incremental development approach**, with each feature added in isolated commits.

---

## ğŸš€ Features (Current Version)

* FastAPI REST API
* Async endpoints using **httpx**
* Background LLM inference using **Celery**
* **Redis** as message broker and result backend
* Task-based request handling with result polling
* Centralized configuration using environment variables
* Returns raw model output including metadata

---

## ğŸ§  Technology Stack (Current)

* Python 3.10+
* FastAPI
* Uvicorn
* httpx
* Celery
* Redis
* pydantic-settings
* Ollama (local LLM runtime)

---

## ğŸ“¦ Prerequisites

* Python 3.10 or higher
* Ollama installed and running
* Docker (recommended for Redis)
* A model pulled in Ollama (example: `llama3`)

### Pull model:

```bash
ollama pull llama3
```

### Start Ollama:

```bash
ollama serve
```

---

## ğŸ”§ Installation

Install dependencies:

```bash
pip install -r requirements.txt
```

Create a `.env` file:

```env
OLLAMA_URL=http://localhost:11434/api/generate
REDIS_URL=redis://localhost:6379/0
```

---

## â–¶ï¸ Running the Application

### 1ï¸âƒ£ Start Redis (Docker recommended)

```bash
docker run -d -p 6379:6379 redis:7
```

---

### 2ï¸âƒ£ Start Celery Worker (Windows âš ï¸)

On **Windows**, Celery must be run using the `solo` pool due to multiprocessing limitations:

```bash
celery -A app.celery_worker worker --loglevel=info --pool=solo
```

> âš ï¸ On Linux/macOS, production deployments typically use `prefork` or `gevent`.

---

### 3ï¸âƒ£ Start FastAPI

```bash
uvicorn app.main:app --reload
```

API available at:

```
http://localhost:8000
```

---

## ğŸ“– API Documentation

Swagger UI:

ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ”¹ API Endpoints

### POST `/generate`

Submit a text generation request.

#### Request

```json
{
  "prompt": "Explain Celery in one sentence"
}
```

#### Response

```json
{
  "task_id": "a72a58c5-96d4-4052-bdb8-80a923faca4f"
}
```

---

### GET `/result/{task_id}`

Retrieve task status and result.

#### Response (Completed)

```json
{
  "status": "done",
  "result": {
    "model": "llama3",
    "response": "Celery is a distributed task queue used for background processing."
  }
}
```

#### Response (Pending)

```json
{
  "status": "pending"
}
```

---

## ğŸ› ï¸ Project Status

âœ… Async FastAPI API
âœ… Celery + Redis background processing
âœ… Windows-compatible Celery worker
âœ… Centralized configuration
âœ… Ready for Docker and MLOps extensions

---

## ğŸ”® Planned Enhancements

* Dockerfile + Docker Compose
* MLflow integration
* Hugging Face model registry
* Authentication & rate limiting
* Streaming responses

---

## ğŸ“Œ Notes

This project is built **step by step** to clearly demonstrate:

* API scalability patterns
* Background job processing
* MLOps-ready architecture

Each major change is introduced in a **separate commit**.